Saving all output to "!!{outputDirectory}!!/stats6.q.raw". Enter "record" with no arguments to stop it.
>>>  !run !!{qFileDirectory}!!/stats6.q
>>>  set datanucleus.cache.collections=false;
No rows affected 
>>>  set hive.stats.autogather=false;
No rows affected 
>>>  set hive.exec.dynamic.partition=true;
No rows affected 
>>>  set hive.exec.dynamic.partition.mode=nonstrict;
No rows affected 
>>>  
>>>  create table analyze_srcpart like srcpart;
No rows affected 
>>>  insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;
'key','value','ds','hr'
No rows selected 
>>>  
>>>  analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;
'key','value','ds','hr'
No rows selected 
>>>  analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=12) compute statistics;
'key','value','ds','hr'
No rows selected 
>>>  
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-08, 11], dbName:stats6, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats6.db/analyze_srcpart/ds=2008-04-08/hr=11, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=500, totalSize=5812, rawDataSize=5312})',''
6 rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-08',hr=12);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-08, 12], dbName:stats6, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats6.db/analyze_srcpart/ds=2008-04-08/hr=12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=500, totalSize=5812, rawDataSize=5312})',''
6 rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-09',hr=11);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-09, 11], dbName:stats6, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats6.db/analyze_srcpart/ds=2008-04-09/hr=11, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=!!UNIXTIME!!})',''
6 rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-09',hr=12);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-09, 12], dbName:stats6, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats6.db/analyze_srcpart/ds=2008-04-09/hr=12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=!!UNIXTIME!!})',''
6 rows selected 
>>>  
>>>  describe extended analyze_srcpart;
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Table Information','Table(tableName:analyze_srcpart, dbName:stats6, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats6.db/analyze_srcpart, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{numPartitions=2, numFiles=2, transient_lastDdlTime=!!UNIXTIME!!, numRows=1000, totalSize=11624, rawDataSize=10624}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
6 rows selected 
>>>  !record
