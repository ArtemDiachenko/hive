Saving all output to "!!{outputDirectory}!!/stats8.q.raw". Enter "record" with no arguments to stop it.
>>>  !run !!{qFileDirectory}!!/stats8.q
>>>  set datanucleus.cache.collections=false;
No rows affected 
>>>  set hive.stats.autogather=false;
No rows affected 
>>>  set hive.exec.dynamic.partition=true;
No rows affected 
>>>  set hive.exec.dynamic.partition.mode=nonstrict;
No rows affected 
>>>  
>>>  create table analyze_srcpart like srcpart;
No rows affected 
>>>  insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;
'key','value','ds','hr'
No rows selected 
>>>  
>>>  explain analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME analyze_srcpart) (TOK_PARTSPEC (TOK_PARTVAL ds '2008-04-08') (TOK_PARTVAL hr 11))))'
''
'STAGE DEPENDENCIES:'
'  Stage-0 is a root stage'
'  Stage-1 depends on stages: Stage-0'
''
'STAGE PLANS:'
'  Stage: Stage-0'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        analyze_srcpart '
'          TableScan'
'            alias: analyze_srcpart'
''
'  Stage: Stage-1'
'    Stats-Aggr Operator'
''
''
19 rows selected 
>>>  analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;
'key','value','ds','hr'
No rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-08, 11], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-08/hr=11, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=500, totalSize=5812, rawDataSize=5312})',''
6 rows selected 
>>>  describe extended analyze_srcpart;
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Table Information','Table(tableName:analyze_srcpart, dbName:stats8, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{numPartitions=1, numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=500, totalSize=5812, rawDataSize=5312}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
6 rows selected 
>>>  
>>>  explain analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=12) compute statistics;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME analyze_srcpart) (TOK_PARTSPEC (TOK_PARTVAL ds '2008-04-08') (TOK_PARTVAL hr 12))))'
''
'STAGE DEPENDENCIES:'
'  Stage-0 is a root stage'
'  Stage-1 depends on stages: Stage-0'
''
'STAGE PLANS:'
'  Stage: Stage-0'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        analyze_srcpart '
'          TableScan'
'            alias: analyze_srcpart'
''
'  Stage: Stage-1'
'    Stats-Aggr Operator'
''
''
19 rows selected 
>>>  analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=12) compute statistics;
'key','value','ds','hr'
No rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-08',hr=12);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-08, 12], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-08/hr=12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=500, totalSize=5812, rawDataSize=5312})',''
6 rows selected 
>>>  
>>>  explain analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=11) compute statistics;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME analyze_srcpart) (TOK_PARTSPEC (TOK_PARTVAL ds '2008-04-09') (TOK_PARTVAL hr 11))))'
''
'STAGE DEPENDENCIES:'
'  Stage-0 is a root stage'
'  Stage-1 depends on stages: Stage-0'
''
'STAGE PLANS:'
'  Stage: Stage-0'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        analyze_srcpart '
'          TableScan'
'            alias: analyze_srcpart'
''
'  Stage: Stage-1'
'    Stats-Aggr Operator'
''
''
19 rows selected 
>>>  analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=11) compute statistics;
'key','value','ds','hr'
No rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-09',hr=11);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-09, 11], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-09/hr=11, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=500, totalSize=5812, rawDataSize=5312})',''
6 rows selected 
>>>  
>>>  explain analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=12) compute statistics;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME analyze_srcpart) (TOK_PARTSPEC (TOK_PARTVAL ds '2008-04-09') (TOK_PARTVAL hr 12))))'
''
'STAGE DEPENDENCIES:'
'  Stage-0 is a root stage'
'  Stage-1 depends on stages: Stage-0'
''
'STAGE PLANS:'
'  Stage: Stage-0'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        analyze_srcpart '
'          TableScan'
'            alias: analyze_srcpart'
''
'  Stage: Stage-1'
'    Stats-Aggr Operator'
''
''
19 rows selected 
>>>  analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=12) compute statistics;
'key','value','ds','hr'
No rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-09',hr=12);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-09, 12], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-09/hr=12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=500, totalSize=5812, rawDataSize=5312})',''
6 rows selected 
>>>  
>>>  explain analyze table analyze_srcpart PARTITION(ds, hr) compute statistics;
'Explain'
'ABSTRACT SYNTAX TREE:'
'  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME analyze_srcpart) (TOK_PARTSPEC (TOK_PARTVAL ds) (TOK_PARTVAL hr))))'
''
'STAGE DEPENDENCIES:'
'  Stage-0 is a root stage'
'  Stage-1 depends on stages: Stage-0'
''
'STAGE PLANS:'
'  Stage: Stage-0'
'    Map Reduce'
'      Alias -> Map Operator Tree:'
'        analyze_srcpart '
'          TableScan'
'            alias: analyze_srcpart'
''
'  Stage: Stage-1'
'    Stats-Aggr Operator'
''
''
19 rows selected 
>>>  analyze table analyze_srcpart PARTITION(ds, hr) compute statistics;
'key','value','ds','hr'
No rows selected 
>>>  
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-08, 11], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-08/hr=11, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, numRows=1000, totalSize=5812, rawDataSize=10624})',''
6 rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-08',hr=12);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-08, 12], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-08/hr=12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, totalSize=5812, numRows=0, rawDataSize=0})',''
6 rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-09',hr=11);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-09, 11], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-09/hr=11, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, totalSize=5812, numRows=0, rawDataSize=0})',''
6 rows selected 
>>>  describe extended analyze_srcpart PARTITION(ds='2008-04-09',hr=12);
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Partition Information','Partition(values:[2008-04-09, 12], dbName:stats8, tableName:analyze_srcpart, createTime:!!UNIXTIME!!, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart/ds=2008-04-09/hr=12, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{numFiles=1, transient_lastDdlTime=!!UNIXTIME!!, totalSize=5812, numRows=0, rawDataSize=0})',''
6 rows selected 
>>>  describe extended analyze_srcpart;
'col_name','data_type','comment'
'key','string',''
'value','string',''
'ds','string',''
'hr','string',''
'','',''
'Detailed Table Information','Table(tableName:analyze_srcpart, dbName:stats8, owner:!!{user.name}!!, createTime:!!UNIXTIME!!, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null), FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], location:!!{hive.metastore.warehouse.dir}!!/stats8.db/analyze_srcpart, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:hr, type:string, comment:null)], parameters:{numPartitions=4, numFiles=4, transient_lastDdlTime=!!UNIXTIME!!, numRows=1000, totalSize=23248, rawDataSize=10624}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)',''
6 rows selected 
>>>  !record
